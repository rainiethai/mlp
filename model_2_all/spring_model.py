# -*- coding: utf-8 -*-
"""Spring Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uH60fuR8d8aVR8VB3juvC0k8gH3iAYv8

## **Importing the libraries and mount Google Drive**
"""

# Import all the necessary libraries
try:
  import pandas as pd
  import time
  import collections
  from google.colab import drive
  import glob
  import tensorflow as tf
  import scipy
  import dask.dataframe as dd
  from itertools import permutations, combinations
  from random import shuffle, choice
  import math
  import os
  import numpy as np
  import matplotlib.pyplot as plt
  from sklearn.model_selection import train_test_split
  from tensorflow import keras
  from sklearn.metrics import r2_score
  from keras.models import Model
  from keras.layers import Input, Dense, TimeDistributed, Bidirectional, Reshape, Flatten
  from keras.layers.recurrent import LSTM
  from keras.layers.merge import concatenate
  from sklearn.metrics import mean_absolute_error
  from tensorflow.keras.models import load_model
  from pyCompare import blandAltman
  from sklearn.preprocessing import MinMaxScaler, StandardScaler
except:
  !pip install pyCompare

# This is to set up authority for uploading files from Google Drive, if training on regular PC, then no need!
drive.mount('/content/gdrive')

# changing the directory to the right place in google
home_dir0 = '/content/gdrive/My Drive/Independent_Study_ML/'
home_dir1 = '/content/gdrive/My Drive/Colab_Notebooks/Independent_Study_ML/'

for dir in [home_dir0, home_dir1]:
  if os.path.exists(dir):
    home_dir = dir
    break

os.chdir(home_dir)

"""## **Loading in the data**"""

# loading in the data
steady = dd.read_csv('steady/*.csv').astype('float16')
pulsatile = dd.read_csv('pulsatile/*.csv',).astype('float16')
steady = steady.drop(['Unnamed: 0','rho', 'wss'], axis=1).compute()
pulsatile = pulsatile.drop(['Unnamed: 0', 'rho', 'wss'], axis=1).compute()

tree_data = scipy.spatial.cKDTree(steady[['x', 'y','z']])
# these are the index of the nearest neighbors
steady_neighbors = tree_data.query(steady[['x', 'y','z']], k=19)[1] # 18 different indexes -> 19 points in total (because we already have the first one, original),  161 info in total because of 19*8 and add 1 to it because of the wall points info
steady_list = pd.DataFrame()
steady_list['neighbors'] = steady_neighbors.tolist()
# will be giving wall and fluid points
# 1) finding the distance away from the wall iwth each position
# 2) geometry representation
# showing a neighborhood
steady.iloc[steady_neighbors[0][1:]]

# a preview of the first 5 rows of steadying data
steady.head(n=100)

pulsatile.head(n=100)

# splitting the data into training and testing sets, test size 10%
training_list, validation_test_list = train_test_split(steady_list, test_size=0.2, random_state=0)
validation_list, test_list = train_test_split(validation_test_list, test_size=0.5, random_state=0)

# stripping the index
training_list.reset_index(drop=True)
validation_list.reset_index(drop=True)
test_list.reset_index(drop=True)

pulsatile.iloc[test_list.iloc[0].values.squeeze().tolist()]

"""## Preparing Data and Generator"""

class DataGenerator(keras.utils.Sequence):
    'Generates data for Keras'
    def __init__(self, list_IDs, steady, pulsatile, batch_size=32, dim=7, n_channels=19, shuffle=True):
        'Initialization'
        self.dim = dim
        self.batch_size = batch_size
        self.steady = steady
        self.pulsatile = pulsatile
        self.list_IDs = list_IDs
        self.n_channels = n_channels
        self.shuffle = shuffle
        self.on_epoch_end()

    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.list_IDs) / self.batch_size))

    def __getitem__(self, index):
        'Generate one batch of data'
        # Generate indexes of the batch
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]

        # Find list of IDs
        list_IDs_temp = [self.list_IDs.iloc[k] for k in indexes]

        # Generate data
        steady, pulsatile = self.__data_generation(list_IDs_temp)
        steady = (steady-np.mean(steady, axis=0))/np.std(steady, axis=0)
        pulsatile = (pulsatile-np.mean(pulsatile, axis=0))/np.std(pulsatile, axis=0)
        return steady, pulsatile

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        self.indexes = np.arange(len(self.list_IDs))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, list_IDs_temp):
        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)
        # Initialization
        steady_df = np.empty((self.batch_size, self.n_channels, self.dim))
        pulsatile_df = np.empty((self.batch_size, self.n_channels, self.dim))

        # Generate data
        for i, ID in enumerate(list_IDs_temp):
            # Store steady
            tempt_steady = self.steady.iloc[ID.values.squeeze().tolist()]
            #tempt_steady = (tempt_steady-tempt_steady.mean())/tempt_steady.std()
            steady_df[i,] = tempt_steady

            # Store pulsatile
            tempt_pulsatile = self.pulsatile.iloc[ID.values.squeeze().tolist()]
            #tempt_pulsatile = (tempt_pulsatile-tempt_pulsatile.mean())/tempt_pulsatile.std()
            pulsatile_df[i,] = tempt_pulsatile
        return steady_df, pulsatile_df

# setting up a run name for logs reference
run_name = 'model_1_all'
use_bestWeight = True

# set a callback to save the best weight
filepath=os.path.join(run_name,"{epoch:02d}_val_mae:{val_mean_absolute_error:.4f}.hdf5")
callback = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')

# set up a callback to reduce the learning rate
reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                              patience=2, min_lr=1e-15, verbose=1)

# early stop training if validation loss is not decreasing
early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', verbose=1, patience=4)

# set up a tensorboard logs
log_dir = f'./logs/{run_name}'

# creating the directories if not exists
for directory in [log_dir, run_name]:
  if not os.path.exists(directory):
    os.makedirs(directory)
tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0, write_graph=True, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch')

# calling the generator
params = {'dim': (6),
          'batch_size': 220,
          'n_channels': 19,
          'shuffle': True}
test_params = {'dim': (6),
          'batch_size': 200,
          'n_channels': 19,
          'shuffle': False}
training_generator = DataGenerator(training_list, steady, pulsatile, **params)
validation_generator = DataGenerator(validation_list, steady, pulsatile, **params)
testing_generator = DataGenerator(test_list, steady, pulsatile, **test_params)

"""**Designing the Model**"""

# create and fit the LSTM network
model_input = Input(shape=(19, 6,), name='input_layer')
model = Dense(500, kernel_initializer='normal', activation='relu')(model_input)
model = Dense(400, kernel_initializer='normal', activation='relu')(model)
model = Dense(300, kernel_initializer='normal', activation='relu')(model)
model = Dense(200, kernel_initializer='normal', activation='relu')(model)
model = Dense(100, kernel_initializer='normal', activation='relu')(model)
output_reshape = Dense(6, kernel_initializer='normal', activation='linear', name='output')(model)

model = Model(inputs=model_input, outputs=output_reshape)

model.summary()
# Open the file
with open(os.path.join(run_name, 'report.txt'), 'w') as fh:
    # Pass the file handle in as a lambda function to make it callable
    model.summary(print_fn=lambda x: fh.write(x + '\n'))

model.compile(loss='mse', optimizer='adam', metrics=['mae', 'mse'])

"""**Training the Model**"""

# loading the weight if training is already done, if not, then train
if use_bestWeight:
    filename = '13_val_mae:0.0553.hdf5'
    model = load_model(os.path.join(run_name, filename))
    model.fit_generator(generator=training_generator, validation_data=validation_generator, use_multiprocessing=True, workers=2, epochs=40 , verbose=1, callbacks=[callback, reduce_lr, tensorboard, early_stop])

"""# **Prediction**

## **Running Prediction**
"""

model.evaluate_generator(testing_generator, workers=2, use_multiprocessing=True, verbose=1, callbacks=None)

test_predict=model.predict_generator(testing_generator, workers=2, use_multiprocessing=True, verbose=1, callbacks=None)

tempt_pulsatile = pulsatile.iloc[test_list.iloc[0].values.squeeze().tolist()]
tempt_pulsatile = (tempt_pulsatile-tempt_pulsatile.mean())/tempt_pulsatile.std()
tempt_pulsatile.iloc[0].to_numpy()

test_predict[0][0]

test_pred_df = []
test_true_df = []
for item in range(10):
  for idx in range(19):
    test_pred_df.extend(test_predict[item][idx])
    tempt_pulsatile = pulsatile.iloc[test_list.iloc[item].values.squeeze().tolist()]
    tempt_pulsatile = (tempt_pulsatile-tempt_pulsatile.mean())/tempt_pulsatile.std()
    test_true_df.extend(tempt_pulsatile.iloc[idx].to_numpy())
blandAltman(test_pred_df, test_true_df,savePath=os.path.join(run_name, 'BA_model1.png'),figureFormat='png')

test_predict[0]

def dice(im1, im2):
    """
    Computes the Dice coefficient, a measure of set similarity.
    Parameters
    ----------
    im1 : array-like, bool
        Any array of arbitrary size. If not boolean, will be converted.
    im2 : array-like, bool
        Any other array of identical size. If not boolean, will be converted.
    Returns
    -------
    dice : float
        Dice coefficient as a float on range [0,1].
        Maximum similarity = 1
        No similarity = 0
        
    Notes
    -----
    The order of inputs for `dice` is irrelevant. The result will be
    identical if `im1` and `im2` are switched.
    """
    im1 = np.asarray(im1).astype(np.bool)
    im2 = np.asarray(im2).astype(np.bool)

    if im1.shape != im2.shape:
        raise ValueError("Shape mismatch: im1 and im2 must have the same shape.")

    # Compute Dice coefficient
    intersection = np.logical_and(im1, im2)

    return 2. * intersection.sum() / (im1.sum() + im2.sum())
dice(test_pred_df, test_true_df)

"""## **Wall**"""

# wall classifcation: wall == 1 and wall ==0
# create a column called 
# magnitude of velocity = x^2 + y^2 + z^2
steady['velocity'] = np.sqrt(steady['ux'].pow(2)+steady['uy'].pow(2)+steady['uz'].pow(2)) # learn adding conditions in dataframe
steady['wall'] = np.where(steady['velocity']<=0.00001, 1, 0)
# a preview of the first 5 rows of label data
steady["wall"].value_counts()

pulsatile['velocity'] = np.sqrt(pulsatile['ux'].pow(2)+pulsatile['uy'].pow(2)+pulsatile['uz'].pow(2)) # learn adding conditions in dataframe
pulsatile['wall'] = np.where(pulsatile['velocity']<=0.00001, 1, 0)
# a preview of the first 5 rows of label data
pulsatile["wall"].value_counts()

"""## **Distance to Wall**"""

# Distance to Wall: 1-sqrt(y^2+z^2)
radius_steady_y = (steady['y'].max()-steady['y'].min())/2 # radius is (max-min)/2
radius_steady_z = (steady['z'].max()-steady['z'].min())/2 # radius is (max-min)/2
if radius_steady_y == radius_steady_z:
  radius_steady = radius_steady_z
else:
  radius_steady = np.mean([radius_steady_y,radius_steady_z])
radius_pulsatile_y = (pulsatile['y'].max()-pulsatile['y'].min())/2 # radius is (max-min)/2
radius_pulsatile_z = (pulsatile['z'].max()-pulsatile['z'].min())/2 # radius is (max-min)/2
if radius_pulsatile_y == radius_pulsatile_z:
  radius_pulsatile = radius_pulsatile_z
else:
  radius_pulsatile = np.mean([radius_pulsatile_z,radius_pulsatile_y])
steady['Dis_to_Wall'] = radius_steady - np.sqrt(steady['y'].pow(2)+steady['z'].pow(2)) # 0.001 m is the radius
pulsatile['Dis_to_Wall'] = radius_pulsatile - np.sqrt(pulsatile['y'].pow(2)+pulsatile['z'].pow(2)) # 0.001 m is the radius

steady.head(n=200)

plt.scatter(range(0,len(steady['y'])), abs(steady['y']), c='blue')
plt.scatter(range(0,len(steady['z'])), abs(steady['z']), c='green')
plt.scatter(range(0,len(steady['Dis_to_Wall'])), steady['Dis_to_Wall'], c='red')
plt.legend(['pos y','pos z', 'Distance to Wall'])
plt.xlabel('Index')
plt.ylabel('Distance [mm]')
plt.title('Steady Distance to Wall')

plt.scatter(range(0,len(pulsatile['y'])), abs(pulsatile['y']), c='blue')
plt.scatter(range(0,len(pulsatile['z'])), abs(pulsatile['z']), c='green')
plt.scatter(range(0,len(pulsatile['Dis_to_Wall'])), pulsatile['Dis_to_Wall'], c='red')
plt.legend(['pos y','pos z', 'Distance to Wall'])
plt.xlabel('Index')
plt.ylabel('Distance [mm]')
plt.title('Pulsatile Distance to Wall')